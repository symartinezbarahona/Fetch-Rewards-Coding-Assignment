{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unzipping .gz files \n",
    "for f in os.listdir():\n",
    "  if 'json' in f:  \n",
    "      with gzip.open(f, 'rb') as f_in:\n",
    "          with open(f.replace('.gz',''), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Wrangling\n",
    "\n",
    "Due to invalid formatting found in the JSON files during data modeling, the code below was utilized to aid in validating and cleaning JSON data for processing. First, run **python -m json.tool filename** in command line to check whether file is a valid JSON document. Should receive an error if invalid. Othewise, the whole file should print. \n",
    "\n",
    "IF an error is confirmed, run the data pipeline below to render a clean json file. The data pipeline is defined to utilze the JSONDecoder.raw_decode() (and its undocumented second parameter) to traverse the data, look for valid JSON structures in an iterative manner, and parse any invalid structures it encounters. A nice benefit to this built-in json module is that it will properly parse the data even if the concatenated JSONs are not properly indented or are just missing. \n",
    "\n",
    "Once all our JSON data has been parsed, the file will be outputted, read again, and unnested at the first level. This should aid us in better idenitifying which JSON objects need to be flatten even further.\n",
    "\n",
    "**Please note that even after running the JSON files into the data pipeline, the data will still be structured as a JSON array (or list in Python) rather than the standard JSON object (or dict in Python)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIX FUNCTION TMR\n",
    "parser = json.JSONDecoder()\n",
    "parsed = []  # a list to hold individually parsed JSON structures\n",
    "\n",
    "def jsonFormatter(filename):\n",
    "    with open('{filename}.json'.format(filename = filename)) as f:\n",
    "        data = f.read()\n",
    "    head = 0  # hold the current position as we parse\n",
    "    while True:\n",
    "        head = (data.find('{', head) + 1 or data.find('[', head) + 1) - 1\n",
    "        try:\n",
    "            struct, head = parser.raw_decode(data, head)\n",
    "            parsed.append(struct)\n",
    "        except (ValueError, json.JSONDecodeError):  # no more valid JSON structures\n",
    "            break\n",
    "\n",
    "    with open('{filename}Clean.json'.format(filename = filename), 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(parsed, jsonfile, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            # Clean.json file is read,, loaded as a list, and then unnested by the first level. This will automatically covert it to a dataframe. \n",
    "    with open('{filename}Clean.json'.format(filename = filename)) as f:\n",
    "        cleanDataDict = json.load(f)\n",
    "        file = pd.json_normalize(cleanDataDict, max_level = 1)\n",
    "        # Due to unnesting, column names include json keys so this function helps with removing them.\n",
    "        file.rename(columns=lambda x: x.split('.')[0].replace(' ','') if '.' in x else x, inplace= True)\n",
    "        return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fetch rewards datasets.\n",
    "#users = jsonFormatter('users')\n",
    "#receipts = jsonFormatter('receipts')\n",
    "#brands = jsonFormatter('brands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = json.JSONDecoder()\n",
    "parsed = []  # a list to hold individually parsed JSON structures\n",
    "\n",
    "#def jsonFormatter(filename):\n",
    "with open('receipts.json') as f:\n",
    "    data = f.read()\n",
    "    head = 0  # hold the current position as we parse\n",
    "    while True:\n",
    "        head = (data.find('{', head) + 1 or data.find('[', head) + 1) - 1\n",
    "        try:\n",
    "            struct, head = parser.raw_decode(data, head)\n",
    "            parsed.append(struct)\n",
    "        except (ValueError, json.JSONDecodeError):  # no more valid JSON structures\n",
    "            break\n",
    "\n",
    "with open('receiptsClean.json', 'w', encoding='utf-8') as jsonfile:\n",
    "    json.dump(parsed, jsonfile, ensure_ascii=False, indent=2)\n",
    "    receipts = pd.json_normalize(parsed, max_level = 1)\n",
    "    # Due to unnesting, column names include json keys so this function helps with removing them.\n",
    "    receipts.rename(columns=lambda x: x.split('.')[0].replace(' ','') if '.' in x else x, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening Deeply Nested JSON Objects\n",
    "\n",
    "After formatting our datasets into a desired state, our next step is to explode the deeply nested variables such as 'rewardsReceiptItemList' in an attempt to access the JSON arrays (or lists of receipts). From there, we ensure each item is embedded within a dictionary, convert them to a string, and feed our variable into the literal_eval function to detect the dictionaries within each receipt. Finally, we run json_normalize to unnest all keys and values and merge them back to their respective datasets by index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts = receipts.explode('rewardsReceiptItemList')\n",
    "receipts.reset_index(inplace=True)\n",
    "\n",
    "receipts = receipts.fillna({'rewardsReceiptItemList':'{}'})\n",
    "receipts['rewardsReceiptItemList'] = receipts['rewardsReceiptItemList'].apply(lambda x:str(x))\n",
    "receipts['rewardsReceiptItemList'] = receipts['rewardsReceiptItemList'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardsReceipts_flat = pd.json_normalize(receipts['rewardsReceiptItemList'],errors='ignore',record_prefix='rewardsReceiptItemList')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_clean =  pd.merge(receipts, rewardsReceipts_flat, left_index = True, right_index = True, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_clean = receipts_clean.drop(['rewardsReceiptItemList'], axis= 1)\n",
    "receipts_clean = receipts_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read json files and make them dataframe for data wrangling\n",
    "#users_test = pd.read_json('users.json', lines= True)\n",
    "#receipts_test = pd.read_json('receipts.json', lines= True)\n",
    "#brands_test = pd.read_json('brands.json', lines= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac7bf7f7fc21f46acede5780e5857257e0a15ccef43417f7f98f93b4afb2bd56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
