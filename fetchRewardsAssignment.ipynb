{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "import sqlalchemy as sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unzipping .gz files \n",
    "for f in os.listdir():\n",
    "  if 'json' in f:  \n",
    "      with gzip.open(f, 'rb') as f_in:\n",
    "          with open(f.replace('.gz',''), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Wrangling\n",
    "\n",
    "In this section, we will be formatting our JSON files, processing nested JSON objects, and cleaning up the overall structure of the datasets.\n",
    "## File Formatting\n",
    "\n",
    "Due to invalid formatting found in the JSON files during data modeling, the code below assist us in validating and cleaning JSON data for processing. First, run **python -m json.tool filename** in the command line to check whether the file is a valid JSON document. You should receive an error if invalid. Othewise, the whole file prints. \n",
    "\n",
    "If an error is confirmed, run the data pipeline below to render a clean json file. The data pipeline is defined to utilze the JSONDecoder.raw_decode() (and its undocumented second parameter) to traverse the data, look for valid JSON structures in an iterative manner, and parse any invalid structures it encounters. A nice benefit to this built-in json module is that it will properly parse the data even if the concatenated JSONs are not properly indented or are just missing. \n",
    "\n",
    "Once all our JSON data has been parsed, the file will be outputted, read again, and unnested at the first level. This should aid us in idenitifying which JSON objects need to be flatten even further.\n",
    "\n",
    "**Please note that even after running the JSON files into the data pipeline, the data will still be structured as a JSON array (or list in Python) rather than the standard JSON object (or dict in Python)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonFormatter(filename, parsed= None, parser= None): \n",
    "    parser = json.JSONDecoder() \n",
    "    parsed = [] # a list to hold individually parsed JSON structures\n",
    "    with open('{filename}.json'.format(filename = filename)) as f: \n",
    "        data = f.read() \n",
    "        head = 0 # hold the current position as we parse while True: \n",
    "        while True:\n",
    "            head = (data.find('{', head) + 1 or data.find('[', head) + 1) - 1\n",
    "            try:\n",
    "                struct, head = parser.raw_decode(data, head)\n",
    "                parsed.append(struct)\n",
    "            except (ValueError, json.JSONDecodeError):  # no more valid JSON structures\n",
    "                break\n",
    "\n",
    "    with open('{filename}Clean.json'.format(filename = filename), 'w', encoding='utf-8') as jsonfile: # Parsed file is outputted for documentation\n",
    "        json.dump(parsed, jsonfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "        df = pd.json_normalize(parsed, max_level = 1) # objects unnested\n",
    "        df.rename(columns=lambda x: x.split('.')[0].replace(' ','') if '.' in x else x, inplace= True) #removing json keys in column name\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fetch rewards datasets.\n",
    "users = jsonFormatter('users')\n",
    "receipts = jsonFormatter('receipts')\n",
    "brands = jsonFormatter('brands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening Deeply Nested JSON Objects\n",
    "\n",
    "After formatting our datasets into a desired state, our next step is to flatten the deeply nested objects and extract the JSON arrays (or lists) that are embedded within each key. From there, we can ensure we have access to the values that will inform our analysis later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receipts\n",
    "\n",
    " In order to access the remaining items nested in the Receipts dataset, we need to explode 'rewardsReceiptItemList' so we can access the lists of receipts. From there, we ensure all the values, especially the NAs, are embedded within the lists so that we can convert them into strings and then feed them into our literal_eval function. Finally, after detecting each dictionary and list,  we run json_normalize to unnest all keys and values and merge them back to their respective datasets by index. In the end, each user should have duplicated rows that represent each individual receipt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts = receipts.reindex(sorted(receipts.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts = receipts.explode('rewardsReceiptItemList') # explode nested objects\n",
    "receipts.reset_index(inplace=True)\n",
    "\n",
    "receipts = receipts.fillna({'rewardsReceiptItemList':'{}'}) # adding curly bracklets to detect lists among NAs\n",
    "receipts['rewardsReceiptItemList'] = receipts['rewardsReceiptItemList'].apply(lambda x:str(x)) # converting to strings\n",
    "receipts['rewardsReceiptItemList'] = receipts['rewardsReceiptItemList'].apply(literal_eval) # detecting dictionaries and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardsReceiptsFlat = pd.json_normalize(receipts['rewardsReceiptItemList'],errors='ignore',record_prefix='rewardsReceiptItemList') # unnesting by variable, ideally performed with meta\n",
    "rewardsReceiptsFlat.rename(columns =  {'pointsEarned': 'pointsEarnedReceipt'}, inplace= True)\n",
    "rewardsReceiptsFlat = rewardsReceiptsFlat.reindex(sorted(rewardsReceiptsFlat.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_clean =  pd.merge(receipts, rewardsReceiptsFlat, left_index = True, right_index = True, how = 'outer') # Merging by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_clean = receipts_clean.drop(['rewardsReceiptItemList', 'index'], axis= 1)\n",
    "receipts_clean.rename(columns = {'_id':'receiptId'}, inplace= True)\n",
    "receipts_clean = receipts_clean.dropna(axis = 1, how = 'all')\n",
    "receipts_clean = receipts_clean.drop_duplicates()\n",
    "#receipts_clean.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brands\n",
    "\n",
    "As a reult of the file formatting, the 'cpg' key was unneested, however, the values for the keys 'id' and 'ref' are now contained in two separate columns with duplicated column names, 'cpg'. To access the values, all we need to do is differentiate the columns and unnest the remaining key, 'oid', which er will merge back in by index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(brands.columns)\n",
    "brands.columns= brands.columns+s.groupby(s).cumcount().replace(0,'').astype(str) # idenitfied duplicated columns and numbered them\n",
    "brands = brands.reindex(sorted(brands.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpgFlatId = pd.json_normalize(brands['cpg'], errors='ignore', record_prefix='cpg' , max_level= 1)\\\n",
    "    .add_prefix('cpgId') #unnesting by variable and adding prefix to column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_clean = pd.merge(brands, cpgFlatId, left_index = True, right_index = True, how = 'outer') # Merging by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_clean = brands_clean.drop(['cpg'], axis= 1)\n",
    "brands_clean.rename(columns =  {'cpg1': 'cpgRef', '_id':'brandId'}, inplace= True)\n",
    "brands_clean.rename(columns=lambda x: x.split('$')[0].replace(' ','') if '$' in x else x, inplace= True) #removing json keys in column name\n",
    "#brands_clean = brands_clean.reindex(sorted(brands_clean.columns), axis=1)\n",
    "brands_clean = brands_clean.dropna(axis = 1, how = 'all')\n",
    "brands_clean = brands_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIX Timestamp to DateTime\n",
    "\n",
    "Now that we have flatten out all the JSON objects in our datasets, we will now convert the unix/epoch time that are present in some of our variables into a regular time stamp. For this purpose, we are going to define a function that divides our unix epoch by 1000 and convert our seconds to a UTC timestamp. This method will apply to both the User and Receipts datasets. \n",
    "\n",
    "Ideally, for bigger datasets, you would want to use pandas.to_datetime() method to convert unix epochs given its faster processing but I forewent this for the sake of customization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateConverter(x):\n",
    "  try:\n",
    "    return(datetime.utcfromtimestamp(x/1000).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "  except:\n",
    "    return pd.NaT #coerce non integers to NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in receipts_clean.columns:\n",
    "    if 'date' in col.lower():\n",
    "        receipts_clean[col] = receipts_clean[col].apply(lambda time: dateConverter(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.reindex(sorted(users.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in users.columns:\n",
    "    if 'date' in col.lower():\n",
    "        users[col] = users[col].apply(lambda time: dateConverter(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_clean = users.drop_duplicates()\n",
    "users_clean = users_clean.dropna(axis = 1, how = 'all')\n",
    "users_clean.rename(columns = {'_id':'userId'}, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Table Designing in SQL\n",
    "\n",
    "In this section... \n",
    "\n",
    "## Inserting Data into SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined function to create url used for creating engines\n",
    "def url(server_name, database):\n",
    "    return(f'mssql+pyodbc://{server_name}/{database}?driver=ODBC Driver 17 for SQL Server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = url('LAPTOP-9Q779EDT','fetchRewards')\n",
    "engine = sal.create_engine(fr)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX ISSUE WITH NVARCHAR!!!!\n",
    "users_clean.to_sql('users', conn, if_exists='replace', index=False, chunksize =5000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sqlalchemy.types import NVARCHAR\n",
    "test.to_sql('receipts', engine, if_exists='replace', index=False);\n",
    "#dtype={col_name: NVARCHAR for col_name in receipts_clean}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_clean.to_sql('brands', conn, if_exists='replace', index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnxn = pyodbc.connect(\"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "                      \"Server=LAPTOP-9Q779EDT;\"\n",
    "                      \"Database=fetchRewards;\"\n",
    "                      \"Trusted_Connection=yes;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PENDING', Decimal('28.032448')),\n",
       " ('FLAGGED', Decimal('2635.570246')),\n",
       " ('FINISHED', Decimal('1244.372934')),\n",
       " ('SUBMITTED', None),\n",
       " ('REJECTED', Decimal('19.544970'))]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = cnxn.cursor()\n",
    "cursor.execute(\"\"\"\n",
    "SELECT r.rewardsReceiptStatus, AVG(CAST([totalSpent] as DECIMAL(9,2))) as avgSpent\n",
    "FROM receipts r\n",
    "GROUP BY r.rewardsReceiptStatus;\"\"\")\n",
    "cursor.fetchall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac7bf7f7fc21f46acede5780e5857257e0a15ccef43417f7f98f93b4afb2bd56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
