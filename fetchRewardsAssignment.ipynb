{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For unzipping .gz files \n",
    "for f in os.listdir():\n",
    "  if 'json' in f:  \n",
    "      with gzip.open(f, 'rb') as f_in:\n",
    "          with open(f.replace('.gz',''), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Wrangling\n",
    "\n",
    "Due to invalid formatting found in the JSON files during data modeling, the code below was utilized to aid in validating and cleaning JSON data for processing. First, run **python -m json.tool filename** in command line to check whether file is a valid JSON document. Should receive an error if invalid. Othewise, the whole file should print. \n",
    "\n",
    "If an error is confirmed, run the data pipeline below to render a clean json file. The data pipeline is defined to utilze the JSONDecoder.raw_decode() (and its undocumented second parameter) to traverse the data, look for valid JSON structures in an iterative manner, and parse any invalid structures it encounters. A nice benefit to this built-in json module is that it will properly parse the data even if the concatenated JSONs are not properly indented or are just missing. \n",
    "\n",
    "Once all our JSON data has been parsed, the file will be outputted, read again, and unnested at the first level. This should aid us in idenitifying which JSON objects need to be flatten even further.\n",
    "\n",
    "**Please note that even after running the JSON files into the data pipeline, the data will still be structured as a JSON array (or list in Python) rather than the standard JSON object (or dict in Python)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonFormatter(filename, parsed= None, parser= None): \n",
    "    parser = json.JSONDecoder() \n",
    "    parsed = [] # a list to hold individually parsed JSON structures\n",
    "    with open('{filename}.json'.format(filename = filename)) as f: \n",
    "        data = f.read() \n",
    "        head = 0 # hold the current position as we parse while True: \n",
    "        while True:\n",
    "            head = (data.find('{', head) + 1 or data.find('[', head) + 1) - 1\n",
    "            try:\n",
    "                struct, head = parser.raw_decode(data, head)\n",
    "                parsed.append(struct)\n",
    "            except (ValueError, json.JSONDecodeError):  # no more valid JSON structures\n",
    "                break\n",
    "\n",
    "    with open('{filename}Clean.json'.format(filename = filename), 'w', encoding='utf-8') as jsonfile: # Parsed file is outputted for documentation\n",
    "        json.dump(parsed, jsonfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "        df = pd.json_normalize(parsed, max_level = 1) # objects unnested\n",
    "        df.rename(columns=lambda x: x.split('.')[0].replace(' ','') if '.' in x else x, inplace= True) #removing json keys in column name\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fetch rewards datasets.\n",
    "users = jsonFormatter('users')\n",
    "receipts = jsonFormatter('receipts')\n",
    "brands = jsonFormatter('brands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flattening Deeply Nested JSON Objects\n",
    "\n",
    "After formatting our datasets into a desired state, our next step is to explode the deeply nested variables such as 'rewardsReceiptItemList' in an attempt to access the JSON arrays (or lists of receipts). From there, we ensure each item, specifically NAs, is embedded within lists, convert them to strings, and feed our variable into the literal_eval function to detect the individualized items within each receipt. Finally, we run json_normalize to unnest all keys and values and merge them back to their respective datasets by index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receipts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts = receipts.reindex(sorted(receipts.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts = receipts.explode('rewardsReceiptItemList') # explode nested objects\n",
    "receipts.reset_index(inplace=True)\n",
    "\n",
    "receipts = receipts.fillna({'rewardsReceiptItemList':'{}'}) #adding curly bracklets to detect lists among NAs\n",
    "receipts['rewardsReceiptItemList'] = receipts['rewardsReceiptItemList'].apply(lambda x:str(x)) # converting to strings\n",
    "receipts['rewardsReceiptItemList'] = receipts['rewardsReceiptItemList'].apply(literal_eval) # detecting dictionaries and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardsReceiptsFlat = pd.json_normalize(receipts['rewardsReceiptItemList'],errors='ignore',record_prefix='rewardsReceiptItemList') #unnesting by variable, ideally performed with meta\n",
    "rewardsReceiptsFlat.rename(columns =  {'pointsEarned': 'pointsEarnedReceipt'}, inplace= True)\n",
    "rewardsReceiptsFlat = rewardsReceiptsFlat.reindex(sorted(rewardsReceiptsFlat.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_clean =  pd.merge(receipts, rewardsReceiptsFlat, left_index = True, right_index = True, how = 'outer') # Merging by index\n",
    "receipts_clean = receipts_clean.drop(['rewardsReceiptItemList'], axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(brands.columns)\n",
    "brands.columns= brands.columns+s.groupby(s).cumcount().replace(0,'').astype(str) # idenitfied duplicated columns and numbered them\n",
    "brands = brands.reindex(sorted(brands.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpgFlatId = pd.json_normalize(brands['cpg'], errors='ignore', record_prefix='cpg' , max_level= 1)\\\n",
    "    .add_prefix('cpgId') #unnesting by variable and adding prefix to column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "brands_clean =  pd.merge(brands, cpgFlatId, left_index = True, right_index = True, how = 'outer') # Merging by index\n",
    "brands_clean = brands_clean.drop(['cpg'], axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIX Timestamp to DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateConverter(x):\n",
    "  try:\n",
    "    return(datetime.utcfromtimestamp(x/1000).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "  except:\n",
    "    return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in receipts_clean.columns:\n",
    "    if 'date' in col.lower():\n",
    "        receipts_clean[col] = receipts_clean[col].apply(lambda time: dateConverter(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_clean = receipts_clean.dropna(axis = 1, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_clean = receipts_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.reindex(sorted(users.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in users.columns:\n",
    "    if 'date' in col.lower():\n",
    "        users[col] = users[col].apply(lambda time: dateConverter(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_clean = users.dropna(axis = 1, how = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_clean = users.drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac7bf7f7fc21f46acede5780e5857257e0a15ccef43417f7f98f93b4afb2bd56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
