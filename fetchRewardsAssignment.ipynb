{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Rewards Coding Project\n",
    "## Overview\n",
    "In this project, I will demonstrate my ability to reason about data as well as showcase how I communicate findings and insights with stakeholders and business executives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Requirements\n",
    "1. Review unstructured JSON data and diagram a new structured relational data model\n",
    "\n",
    "2. Generate a query that answers a predetermined business question\n",
    "    - What are the top 5 brands by receipts scanned for most recent month?\n",
    "    - How does the ranking of the top 5 brands by receipts scanned for the recent month compare to the ranking for the previous month?\n",
    "    - When considering average spend from receipts with 'rewardsReceiptStatus’ of ‘Accepted’ or ‘Rejected’, which is greater?\n",
    "    - When considering total number of items purchased from receipts with 'rewardsReceiptStatus’ of ‘Accepted’ or ‘Rejected’, which is greater?\n",
    "    - Which brand has the most spend among users who were created within the past 6 months?\n",
    "    - Which brand has the most transactions among users who were created within the past 6 months?\n",
    ">     \n",
    "3. Generate a query to capture data quality issues against the new structured relational data model\n",
    "\n",
    "4. Write a short email or Slack message to the business stakeholder (Found outside of notebook)\n",
    "    - What questions do you have about the data?\n",
    "    - How did you discover the data quality issues?\n",
    "    - What do you need to know to resolve the data quality issues?\n",
    "    - What other information would you need to help you optimize the data assets you're trying to create?\n",
    "    - What performance and scaling concerns do you anticipate in production and how do you plan to address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Data\n",
    "\n",
    "### Receipts Data Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column | Definition\n",
    "--- | -----------\n",
    "_id | uuid for this receipt\n",
    "bonusPointsEarned | Number of bonus points that were awarded upon receipt completion\n",
    "createDate | The date that the event was created\n",
    "dateScanned | Date that the user scanned their receipt\n",
    "finishedDate | Date that the receipt finished processing\n",
    "modifyDate | The date the event was modified\n",
    "pointsAwardedDate | The date we awarded points for the transaction\n",
    "pointsEarned | The number of points earned for the receipt\n",
    "purchaseDate | The date of the purchase\n",
    "purchasedItemCount | Count of number of items on the receipt\n",
    "rewardsReceiptItemList | The items that were purchased on the receipt\n",
    "rewardsReceiptStatus | Status of the receipt through receipt validation and processing\n",
    "totalSpent | The total amount on the receipt\n",
    "userId | String id back to the User collection for the user who scanned the receipt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "### Users Data Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column | Definition\n",
    "--- | -----------\n",
    "_id | User Id\n",
    "state | state abbreviation\n",
    "createdDate | When the user created their account\n",
    "lastLogin | Last time the user was recorded logging in to the app\n",
    "role | Constant value set to 'CONSUMER'\n",
    "active | Indicates if the user is active; only Fetch will de-activate an account with this flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "----\n",
    "\n",
    "### Brand Data Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column | Definition\n",
    "--- | -----------\n",
    "_id | Brand uuid\n",
    "barcode | The barcode on the item\n",
    "brandCod | String that corresponds with the brand column in a partner product file\n",
    "category | The category name for which the brand sells products in\n",
    "categoryCode | The category code that references a BrandCategory\n",
    "cpg | Reference to CPG collection\n",
    "topBrand | Boolean indicator for whether the brand should be featured as a 'top brand'\n",
    "name |Brand name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pyodbc\n",
    "import sqlalchemy as sal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir(): # For unzipping .gz files \n",
    "  if 'json' in f:  \n",
    "      with gzip.open(f, 'rb') as f_in:\n",
    "          with open(f.replace('.gz',''), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part One: Review Existing Unstructured Data and Diagram a New Structured Relational Data Model\n",
    "\n",
    "In this section, we will be formatting our JSON files, processing nested JSON objects, specifying our column datatypes, and inserting our data into SQL Server. \n",
    "\n",
    "## File Formatting\n",
    "\n",
    "Due to invalid formatting found in the JSON files during data modeling, the code below assist us in validating and cleaning JSON data for processing. First, run *python -m json.tool filename* in the command line to check whether the file is a valid JSON document. You should receive an error if invalid. Othewise, the whole file prints. \n",
    "\n",
    "If an error is confirmed, run the data pipeline below to render a clean json file. The data pipeline is defined to utilze the JSONDecoder.raw_decode() (and its undocumented second parameter) to traverse the data, look for valid JSON structures in an iterative manner, and parse any invalid structures it encounters. A nice benefit to this built-in json module is that it will properly parse the data even if the concatenated JSONs are not properly indented or are just missing. \n",
    "\n",
    "Once all our JSON data has been parsed, the file will be outputted, read again, and unnested at the first level. This should aid us in idenitifying which JSON objects need to be flatten even further.\n",
    "\n",
    "*Please note that even after running the JSON files into the data pipeline, the data will still be structured as a JSON array (or list in Python) rather than the standard JSON object (or dict in Python)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonFormatter(filename, parsed= None, parser= None): \n",
    "    parser = json.JSONDecoder() \n",
    "    parsed = [] # a list to hold individually parsed JSON structures\n",
    "    with open('{filename}.json'.format(filename = filename)) as f: \n",
    "        data = f.read() \n",
    "        head = 0 # hold the current position as we parse while True: \n",
    "        while True:\n",
    "            head = (data.find('{', head) + 1 or data.find('[', head) + 1) - 1\n",
    "            try:\n",
    "                struct, head = parser.raw_decode(data, head)\n",
    "                parsed.append(struct)\n",
    "            except (ValueError, json.JSONDecodeError):  # no more valid JSON structures\n",
    "                break\n",
    "\n",
    "    with open('{filename}Clean.json'.format(filename = filename), 'w', encoding='utf-8') as jsonfile: # Parsed file is outputted for documentation\n",
    "        json.dump(parsed, jsonfile, ensure_ascii=False, indent=2)\n",
    "\n",
    "        df = pd.json_normalize(parsed, max_level = 1) # objects unnested\n",
    "        df.rename(columns=lambda x: x.split('.')[0].replace(' ','') if '.' in x else x, inplace= True) #removing json keys in column name\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fetch rewards datasets.\n",
    "users = jsonFormatter('users')\n",
    "receipts = jsonFormatter('receipts')\n",
    "brands = jsonFormatter('brands')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening Deeply Nested JSON Objects\n",
    "\n",
    "After formatting our datasets into a desired state, our next step is to flatten the deeply nested objects and extract the JSON arrays (or lists) that are embedded within each key. From there, we can ensure we have access to the values that will inform our analysis later on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receipts\n",
    "\n",
    " In order to access the remaining items nested in the Receipts dataset, we need to explode 'rewardsReceiptItemList' so we can access the lists of receipts. From there, we ensure all the values, especially the NAs, are embedded within the lists so that we can convert them into strings and then feed them into our literal_eval function. Finally, after detecting each dictionary and list,  we run json_normalize to unnest all keys and values and merge them back to their respective datasets by index. In the end, each 'receiptId' should have duplicated rows that represent each individual item by receipt. This was done to more easily extract brand and item information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts = receipts.reindex(sorted(receipts.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts = receipts.explode('rewardsReceiptItemList') # explode nested objects\n",
    "receipts.reset_index(inplace=True)\n",
    "\n",
    "receipts = receipts.fillna({'rewardsReceiptItemList':'{}'}) # adding curly bracklets to detect lists among NAs\n",
    "receipts['rewardsReceiptItemList'] = receipts['rewardsReceiptItemList'].apply(lambda x:str(x)) # converting to strings\n",
    "receipts['rewardsReceiptItemList'] = receipts['rewardsReceiptItemList'].apply(literal_eval) # detecting dictionaries and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardsReceiptsFlat = pd.json_normalize(receipts['rewardsReceiptItemList'],errors='ignore',record_prefix='rewardsReceiptItemList') # unnesting by variable, ideally performed with meta\n",
    "rewardsReceiptsFlat.rename(columns =  {'pointsEarned': 'pointsEarnedReceipt'}, inplace= True) # to avoid duplication\n",
    "rewardsReceiptsFlat = rewardsReceiptsFlat.reindex(sorted(rewardsReceiptsFlat.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiptsClean =  pd.merge(receipts, rewardsReceiptsFlat, left_index = True, right_index = True, how = 'outer') # Merging by index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiptsClean = receiptsClean.drop(['rewardsReceiptItemList', 'index'], axis= 1)\n",
    "receiptsClean.rename(columns = {'_id':'receiptId'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_obj(col): # Function to remove whitespace\n",
    "    if col.dtypes == object:\n",
    "        return (col.astype(str)\n",
    "                   .str.strip()\n",
    "                   .replace({'nan': np.nan}))\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiptsClean = receiptsClean.apply(strip_obj, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiptsClean = receiptsClean.dropna(axis = 1, how = 'all')\n",
    "receiptsClean = receiptsClean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brands\n",
    "\n",
    "As a reult of the file formatting, the 'cpg' key was unneested, however, the values for the keys 'id' and 'ref' are now contained in two separate columns with duplicated column names, 'cpg'. To access the values, all we need to do is differentiate the columns and unnest the remaining key, 'oid', which er will merge back in by index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(brands.columns)\n",
    "brands.columns= brands.columns+s.groupby(s).cumcount().replace(0,'').astype(str) # idenitfied duplicated columns and numbered them\n",
    "brands = brands.reindex(sorted(brands.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpgFlatId = pd.json_normalize(brands['cpg'], errors='ignore', record_prefix='cpg' , max_level= 1)\\\n",
    "    .add_prefix('cpgId') #unnesting by variable and adding prefix to column name for distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandsClean = pd.merge(brands, cpgFlatId, left_index = True, right_index = True, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandsClean = brandsClean.drop(['cpg'], axis= 1)\n",
    "brandsClean.rename(columns =  {'cpg1': 'cpgRef', '_id':'brandId'}, inplace= True)\n",
    "brandsClean.rename(columns=lambda x: x.split('$')[0].replace(' ','') if '$' in x else x, inplace= True) #removing json keys in column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandsClean = brandsClean.apply(strip_obj, axis=0)\n",
    "brandsClean = brandsClean.replace('', np.nan, regex=True) # converted empty spaces into nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandsClean = brandsClean.dropna(axis = 1, how = 'all')\n",
    "brandsClean = brandsClean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNIX Timestamp to DateTime\n",
    "\n",
    "Now that we have flatten out all the JSON objects in our datasets, we will now convert the unix/epoch time that are present in some of our variables into a regular time stamp. For this purpose, we are going to define a function that divides our unix epoch by 1000 and convert our seconds to a UTC timestamp. From there we will use pandas.to_datetime() to convert our timestamps to datetime types. This method will apply to both the user and receipts datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateConverter(x):\n",
    "  try:\n",
    "    return(datetime.utcfromtimestamp(x/1000).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "  except:\n",
    "    return pd.NaT #coerce non integers to NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in receiptsClean.columns:\n",
    "    if 'date' in col.lower():\n",
    "        receiptsClean[col] = receiptsClean[col].apply(lambda date: dateConverter(date)).apply(pd.to_datetime) # applied to convert data to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = users.reindex(sorted(users.columns), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in users.columns:\n",
    "    if any(x in col.lower() for x in ['date', 'login']):\n",
    "        users[col] = users[col].apply(lambda date: dateConverter(date)).apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersClean = users.apply(strip_obj, axis=0)\n",
    "usersClean.rename(columns = {'_id':'userId'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersClean = usersClean.drop_duplicates()\n",
    "usersClean = usersClean.dropna(axis = 1, how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Designing in SQL\n",
    "\n",
    "In order to insert our data into a database that we can later leverage for analysis, we need to specify the datatypes that each of our columns will contain. Otherwise, pandas.to_sql() will just assume everything is nvarchar(max) which greatly affects performance and data integrity. To do this, we are going to define a function that utilizes the datatypes of our pandas.dataframes and updates them to sqlalchemy datatypes. From there, we will pass our columns to our dtype argument which will resolve our nvarchar(max) issue and make our tables usable for querying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url(server_name, database): # Defined function to create url used for creating engines\n",
    "    return(f'mssql+pyodbc://{server_name}/{database}?driver=ODBC Driver 17 for SQL Server')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = url('LAPTOP-9Q779EDT','fetchRewards')\n",
    "engine = sal.create_engine(fr)\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "usersClean.info()\n",
    "receiptsClean.info()\n",
    "brandsClean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in receiptsClean.columns: # Converting strings to float \n",
    "    if any(x in col.lower() for x in ['price', 'total']):\n",
    "        receiptsClean[col] = receiptsClean[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiptsFloat = ['pointsEarned','pointsEarnedReceipt' ]\n",
    "receiptsClean[receiptsFloat] = receiptsClean[receiptsFloat].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlcol(df, dtypedict = None):    # To use dtype with to_sql(), we pass a dictionary keyed to each data frame column with corresponding sqlalchemy types.\n",
    "    dtypedict = {}\n",
    "    for i,j in zip(df.columns, df.dtypes):\n",
    "        if \"object\" in str(j):\n",
    "            dtypedict.update({i: sal.types.VARCHAR(length=255)})                       \n",
    "        if \"datetime\" in str(j):\n",
    "            dtypedict.update({i: sal.types.DateTime()})\n",
    "        if \"float\" in str(j):\n",
    "            dtypedict.update({i: sal.types.Float(precision=3, asdecimal=True)})\n",
    "        if \"int\" in str(j):\n",
    "            dtypedict.update({i: sal.types.INT()})\n",
    "        if \"bool\" in str(j):\n",
    "            dtypedict.update({i: sal.types.VARCHAR(length=255)})\n",
    "    return dtypedict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersType = sqlcol(usersClean)\n",
    "usersClean.to_sql('users', conn, if_exists='replace', index=False, chunksize =5000, dtype = usersType); #Created a new table if it does not exist otherwise drop before inserting data frame values to this table. \n",
    "# Chunksize is used to specify the number of rows to be wrriten in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiptsType =sqlcol(receiptsClean)\n",
    "receiptsClean.to_sql('receipts', conn, if_exists='replace', index=False, chunksize= 5000, dtype = receiptsType);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandsType =sqlcol(brandsClean)\n",
    "brandsClean.to_sql('brands', conn, if_exists='replace', index=False, chunksize= 5000, dtype = brandsType);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part Two: Write a query that directly answers a predetermined question from a business stakeholder\n",
    "\n",
    "In this section, we will anwser several predetermined questions using our newly developed tables and ensure that additional considerations overlooked are taken into account. \n",
    "\n",
    "## Querying our Data\n",
    "\n",
    "To find out which brands have the most spend and transactions among users that were created within the past 6 months, we first need to define a date that we can use as reference for limiting the scope of our users. From there, we join this table with our receipts table and then group our brands by the sum of our 'totalSpent' and 'purchasedItemCount' to get our desired fields. Finally, we will use this data to create visuals using ThinkCell in Microsoft Powerpoint to define the overall spending and transaction share of the top five brands compared to the rest of the brands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "brandsSpendTrans= pd.DataFrame(pd.read_sql_query(\"\"\"\\\n",
    "DECLARE @createdDate DATETIME = (SELECT MAX(createdDate) FROM users);\n",
    "\n",
    "with receipt_uniq as\n",
    "\t(SELECT DISTINCT receiptId,brandCode,totalSpent, purchasedItemCount, userId\n",
    "\tFROM receipts), \n",
    "top5 as \n",
    "\t(SELECT DISTINCT brandCode\n",
    "\t\t,SUM(totalSpent) OVER(PARTITION BY brandCode) [spend]\n",
    "\t\t,SUM(purchasedItemCount) OVER(PARTITION BY brandCode)[transactions]\n",
    "\tFROM receipt_uniq\n",
    "\n",
    "\tINNER JOIN\n",
    "\t(SELECT userId, createdDate\n",
    "\tFROM users\n",
    "\tWHERE DATEDIFF(MONTH, createdDate, @createdDate) between 0 and 6 )\n",
    "\tusers on users.userId = receipt_uniq.userId)\n",
    "\n",
    "SELECT brandCode, \n",
    "ROUND([spend],1) [spend], \n",
    "ROUND([transactions],1) [transactions]\n",
    "FROM top5\n",
    "ORDER BY transactions desc\n",
    "\"\"\", conn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five brands by spend among users who were created within the past 6 months\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brandCode</th>\n",
       "      <th>spend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BEN AND JERRYS</td>\n",
       "      <td>20040.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PEPSI</td>\n",
       "      <td>19390.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOLGERS</td>\n",
       "      <td>18900.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KELLOGG'S</td>\n",
       "      <td>18555.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BIGELOW</td>\n",
       "      <td>18362.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        brandCode     spend\n",
       "1  BEN AND JERRYS  20040.77\n",
       "2           PEPSI  19390.63\n",
       "4         FOLGERS  18900.04\n",
       "6       KELLOGG'S  18555.40\n",
       "7         BIGELOW  18362.71"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top five brands by spend among users who were created within the past 6 months\")\n",
    "top5 = brandsSpendTrans.dropna() # drop null to accurately depict top brands in our tables below\n",
    "top5.nlargest(5,'spend')[['brandCode', 'spend']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top five brands by transactions among users who were created within the past 6 months\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brandCode</th>\n",
       "      <th>transactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BEN AND JERRYS</td>\n",
       "      <td>4111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PEPSI</td>\n",
       "      <td>3937.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DOLE</td>\n",
       "      <td>3748.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOLGERS</td>\n",
       "      <td>3681.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNORR</td>\n",
       "      <td>3674.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        brandCode  transactions\n",
       "1  BEN AND JERRYS        4111.0\n",
       "2           PEPSI        3937.0\n",
       "3            DOLE        3748.0\n",
       "4         FOLGERS        3681.0\n",
       "5           KNORR        3674.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top five brands by transactions among users who were created within the past 6 months\")\n",
    "top5.nlargest(5,'transactions')[['brandCode', 'transactions']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part Three: Evaluate Data Quality Issues in the Data Provided\n",
    "\n",
    "In this section, we will look at one data quality issue that is of upmost importance to address in order to establish proper relational integrity.\n",
    "\n",
    "## Examining the Relationship Between 'brandCodes' in the Brands and Receipts Table.\n",
    "\n",
    "Part of the reason why developing a proper relational database with primary and foreign keys was not feasible was because there was a lot of missing data between the users, receipts, and brands datasets. In fact, without having proper knowledge of the API process and/or the schemas, it became quite difficult to remove fields without actually affecting the overall integrity of the data. Thus, I focused on having a minimal viable product that could allow me to extract meaningful insights while at the same time ensuring that the analysis would not end up skewed or inaccurate. \n",
    "\n",
    "### Brands\n",
    "\n",
    "With this being said, one of the issues that needs to be tackled is the weak relationship that exists between the 'brandCode' field in the brands and receipts datasets. Starting with brands, I first looked at the overall counts of the brandCode field, noticed that there was more overall brandCodes than distinct ones (which shouldn't be the case given they should all be unique), and then followed up by dissecting 'nulls', 'test brands', and joinable values. In the end, when removing null and 'test brands', we are left with a little less than half of our values being joinable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of values in 'brandCode' field in brands\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Brands</th>\n",
       "      <th>Total Distinct Brands</th>\n",
       "      <th>Total Nulls</th>\n",
       "      <th>Total Test Brands</th>\n",
       "      <th>Total Joinable Brands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>898</td>\n",
       "      <td>895</td>\n",
       "      <td>269</td>\n",
       "      <td>359</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Brands  Total Distinct Brands  Total Nulls  Total Test Brands  \\\n",
       "0           898                    895          269                359   \n",
       "\n",
       "   Total Joinable Brands  \n",
       "0                    536  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brandsResults= pd.DataFrame(pd.read_sql_query(\"\"\"\\\n",
    "SELECT COUNT([brandCode]) [Total Brands], \n",
    "    [Total Distinct Brands], \n",
    "    [Total Nulls], \n",
    "    [Total Test Brands], \n",
    "    [Total Joinable Brands]\n",
    "FROM brands\n",
    "\n",
    "cross join \n",
    "(SELECT COUNT(DISTINCT [brandCode]) [Total Distinct Brands]\n",
    "FROM brands) Dist\n",
    "\n",
    "cross join\n",
    "(SELECT SUM(CASE WHEN brandCode IS NULL THEN 1 ELSE 0 END) [Total Nulls]\n",
    "FROM brands) nulls\n",
    "\n",
    "cross join\n",
    "(SELECT COUNT(DISTINCT brandCode) [Total Test Brands]\n",
    "FROM brands\n",
    "where brandCode like 'test%') test\n",
    "\n",
    "cross join\n",
    "(SELECT COUNT(DISTINCT brandCode) [Total Joinable Brands]\n",
    "FROM brands\n",
    "where brandCode not like 'test%' and brandCode is not null ) actual\n",
    "\n",
    "GROUP BY [Total Distinct Brands], [Total Nulls], [Total Test Brands],[Total Joinable Brands];\n",
    "\"\"\", conn))\n",
    "print(\"Count of values in 'brandCode' field in brands\")\n",
    "brandsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receipts\n",
    "\n",
    "Since out receipts table is formatted by individual items, the number of brands listed is quite larger, however, when looking at distinct brands the number dramatically decreases because a majority of them are either duplicates or nulls. This poses a greater challenege because this limits our total joinable values to only 227. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of values in 'brandCode' field in receipts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Receipt Brands</th>\n",
       "      <th>Total Distinct Receipt Brands</th>\n",
       "      <th>Total Receipt Nulls</th>\n",
       "      <th>Total Joinable Receipt Brands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2600</td>\n",
       "      <td>227</td>\n",
       "      <td>4781</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Receipt Brands  Total Distinct Receipt Brands  Total Receipt Nulls  \\\n",
       "0                  2600                            227                 4781   \n",
       "\n",
       "   Total Joinable Receipt Brands  \n",
       "0                            227  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receiptsResults= pd.DataFrame(pd.read_sql_query(\"\"\"\\\n",
    "SELECT COUNT([brandCode]) [Total Receipt Brands], \n",
    "    [Total Distinct Receipt Brands], \n",
    "    [Total Receipt Nulls], \n",
    "    [Total Joinable Receipt Brands]\n",
    "FROM receipts\n",
    "\n",
    "cross join \n",
    "(SELECT COUNT(DISTINCT [brandCode]) [Total Distinct Receipt Brands]\n",
    "FROM receipts) Dist\n",
    "\n",
    "cross join\n",
    "(SELECT SUM(CASE WHEN brandCode IS NULL THEN 1 ELSE 0 END) [Total Receipt Nulls]\n",
    "FROM receipts) nulls\n",
    "\n",
    "cross join\n",
    "(SELECT COUNT(DISTINCT brandCode) [Total Joinable Receipt Brands]\n",
    "FROM receipts\n",
    "where brandCode is not null) actual\n",
    "\n",
    "GROUP BY [Total Distinct Receipt Brands], [Total Receipt Nulls], [Total Joinable Receipt Brands];\n",
    "\"\"\", conn))\n",
    "print(\"Count of values in 'brandCode' field in receipts\")\n",
    "receiptsResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching brandCode \n",
    "\n",
    "Given the number of nulls and 'test brands' found in both our datasets, the total number of matching brands only comes down to a staggering 41 brands. This, of course, greatly limits our ability to develop a proper relationship database and by extension extract meaning insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of matching brands between brands and receipts table\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Matching Brands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Matching Brands\n",
       "0               41"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matchResults= pd.DataFrame(pd.read_sql_query(\"\"\"\\\n",
    "SELECT COUNT(DISTINCT brands.brandCode) as [Matching Brands]\n",
    "FROM brands\n",
    "\n",
    "INNER JOIN\n",
    "(SELECT DISTINCT brandCode\n",
    "FROM receipts) \n",
    "receipts ON receipts.brandCode = brands.brandCode;\n",
    "\"\"\", conn))\n",
    "print(\"Count of matching brands between brands and receipts table\")\n",
    "matchResults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac7bf7f7fc21f46acede5780e5857257e0a15ccef43417f7f98f93b4afb2bd56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
